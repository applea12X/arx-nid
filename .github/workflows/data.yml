name: Data Management

on:
  push:
    branches: [ main ]
  pull_request:
    branches: [ main ]
  workflow_dispatch:

jobs:
  data-validation:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wireshark-common tshark
        
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Configure DVC
      run: |
        dvc config core.no_scm true
        
    - name: Pull data from DVC remote
      run: |
        # Only pull if DVC files exist and we have credentials
        if [ -f .dvc/config ]; then
          echo "Pulling data from DVC remote..."
          dvc pull --quiet || echo "No data to pull or remote not accessible"
        else
          echo "No DVC configuration found"
        fi
      
    - name: Lint code
      run: |
        black --check arx_nid/ scripts/ tests/
        ruff check arx_nid/ scripts/ tests/
        
    - name: Run unit tests
      run: |
        python -m pytest tests/ -v --tb=short
        
    - name: Test feature processing (if data available)
      run: |
        # Test feature transformers with mock data
        python -c "
        import sys
        sys.path.append('.')
        from tests.test_transformers import sample_flow_data
        from arx_nid.features.transformers import FlowPreprocessor
        import pandas as pd
        
        # Create sample data
        data = {
            'ts': [1704067200 + i for i in range(5)],
            'id.orig_h': ['192.168.1.10'] * 5,
            'id.resp_h': ['10.0.0.50'] * 5,
            'proto': ['tcp'] * 5,
            'service': ['http'] * 5,
            'orig_bytes': [100, 200, 300, 400, 500],
            'resp_bytes': [50, 100, 150, 200, 250],
            'orig_pkts': [10, 20, 30, 40, 50],
            'resp_pkts': [5, 10, 15, 20, 25],
            'duration': [1.0, 2.0, 3.0, 4.0, 5.0],
            'conn_state': ['SF'] * 5
        }
        df = pd.DataFrame(data)
        
        preprocessor = FlowPreprocessor()
        result = preprocessor.fit_transform(df)
        print(f'✓ Feature processing successful: {result.shape}')
        "
        
    - name: Rebuild features (if data available)
      run: |
        # Try to rebuild features if we have raw data
        if [ -d "data/raw" ] && [ "$(ls -A data/raw)" ]; then
          echo "Raw data found, attempting feature rebuild..."
          dvc repro build-features --quiet || echo "Feature rebuild failed or no raw data"
        else
          echo "No raw data available for feature rebuild"
        fi
        
    - name: Validate data manifest
      run: |
        python -c "
        import csv
        print('Validating data manifest...')
        with open('data/datasets.csv') as f:
            reader = csv.DictReader(f)
            datasets = list(reader)
            print(f'Found {len(datasets)} datasets in manifest')
            for ds in datasets:
                print(f'  - {ds[\"dataset\"]}: {ds[\"url\"]}')
        "
        
    - name: Check download scripts
      run: |
        echo "Checking download scripts..."
        for script in scripts/download_*.py; do
          if [ -f "$script" ]; then
            echo "✓ $script exists"
            python -m py_compile "$script"
          fi
        done
        
    - name: Validate data structure
      run: |
        echo "Validating data directory structure..."
        ls -la data/
        echo "Raw data directory:"
        ls -la data/raw/ || echo "No raw data directory yet"
        echo "Processed data directory:"
        ls -la data/processed/ || echo "No processed data directory yet"
